{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dc88dd",
   "metadata": {},
   "source": [
    "# **Trabalho Prático: Guia de Viagem Inteligente com Roteamento de Cadeias**\n",
    "\n",
    "**Disciplina:** Inteligência artificial  \n",
    "**Professor:** Luiz Gustavo  \n",
    "**Aluno:** Anderson Paes Gomes  \n",
    "\n",
    "Este notebook implementa a solução para o trabalho proposto que deve atender os requisitos abaixo:\n",
    "\n",
    "## 1. Objetivo do Projeto\n",
    "\n",
    "O principal objetivo é desenvolver um sistema que, a partir de uma consulta de um turista, seja capaz de **classificar a intenção da pergunta** e **direcioná-la para a cadeia de processamento mais adequada**. Isso permite a criação de roteiros de viagem personalizados e a resposta a perguntas específicas de forma rápida e eficiente. O sistema deve demonstrar as seguintes vantagens:\n",
    "\n",
    "- **Roteiros personalizados:** Gerar sugestões de itinerário com base no perfil do turista (ex: aventura, cultural, gastronômico).\n",
    "\n",
    "- **Módulos especializados por função:** Utilizar cadeias de processamento dedicadas a diferentes tipos de consulta (ex: informações sobre locais, logística de transporte, detalhes de roteiro). **Atenção:** Neste projeto, **não serão utilizados Agentes** no sentido da classe Agent do LangChain, mas sim **Cadeias (Chains)** que realizam funções específicas, orquestradas por um roteador.\n",
    "\n",
    "- **Geração rápida e automatizada:** Otimizar a velocidade de resposta utilizando um modelo de inferência de alto desempenho como o Groq.\n",
    "\n",
    "- **Estrutura escalável e personalizável:** A arquitetura modular deve permitir a fácil adição de novas funcionalidades ou bases de conhecimento.\n",
    "\n",
    "## 2. Tecnologias Envolvidas\n",
    "\n",
    "Para a realização deste trabalho, vocês deverão utilizar as seguintes ferramentas e conceitos:\n",
    "\n",
    "- **LangChain:** Como framework principal para orquestrar as cadeias de processamento e a lógica de roteamento.\n",
    "\n",
    "- **Groq:** Otimizado para alta velocidade, será o motor de inferência para o Large Language Model (LLM) que irá gerar as respostas e os roteiros.\n",
    "\n",
    "- **Router Chain (LangChain):** O componente central do projeto. Ele será responsável por analisar a consulta do usuário e decidir qual cadeia especializada deve processá-la.\n",
    "\n",
    "- **Pinecone:** Servirá como base de dados vetorial para implementar o **RAG**. Vocês deverão popular o Pinecone com informações sobre destinos turísticos, pontos de interesse, restaurantes e eventos locais.\n",
    "\n",
    "- **RAG (Retrieval-Augmented Generation):** Essencial para que o sistema possa acessar informações atualizadas e específicas que não estão no conhecimento pré-treinado do LLM, garantindo a precisão das respostas.\n",
    "\n",
    "## 3. Estrutura do Projeto\n",
    "\n",
    "O sistema deverá ser construído em módulos lógicos:\n",
    "\n",
    "- **Módulo de Entrada:** Recebe a consulta do usuário. A consulta deve incluir o tipo de viagem desejada (ex: \"roteiro cultural em Paris por 3 dias\", \"como chegar ao Coliseu?\", \"quais são os melhores restaurantes veganos em Tóquio?\").\n",
    "\n",
    "- **Router Chain:** Classifica a intenção do usuário. As classificações podem ser, por exemplo:\n",
    "\n",
    "    - **roteiro-viagem**\n",
    "    - **logistica-transporte**\n",
    "    - **info-local**\n",
    "    - **traducao-idiomas**\n",
    "\n",
    "- **Cadeias Especializadas:**\n",
    "\n",
    "    - **Itinerary Chain (roteiro-viagem):** Recebe o perfil do turista (se disponível), utiliza o RAG para buscar informações sobre atrações e eventos e gera um roteiro detalhado.\n",
    "    - **Logistics Chain (logistica-transporte):** Responde a perguntas sobre transporte, acomodação e outros aspectos práticos da viagem.\n",
    "    - **Local Info Chain (info-local):** Fornece informações específicas sobre pontos turísticos, restaurantes, horários de funcionamento, etc., usando o RAG.\n",
    "    - **Translation Chain (traducao-idiomas): (Bônus)** Esta cadeia implementará um guia de tradução, fornecendo frases úteis para a viagem com base na solicitação do usuário.\n",
    "\n",
    "- **Base de Conhecimento (RAG):** Crie um conjunto de dados de exemplo sobre uma ou duas cidades turísticas (ex: Rio de Janeiro e Paris) para ser indexado no Pinecone. As informações devem ser relevantes para o turismo (pontos turísticos, restaurantes, dicas de segurança, etc.).\n",
    "\n",
    "## 5. Requisitos e Entrega\n",
    "\n",
    "- **Código-fonte:** Um repositório no GitHub contendo todo o código do projeto. O código deve ser bem comentado e organizado.\n",
    "\n",
    "- **Demonstração:** Um vídeo de 1 a 3 minutos mostrando a funcionalidade do sistema em ação, demonstrando os diferentes fluxos de roteamento e a capacidade de RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e08df3f",
   "metadata": {},
   "source": [
    "### Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056e4c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projetos\\Langchain\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71f53e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1369eeee",
   "metadata": {},
   "source": [
    "**Configuração das Chaves de API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc52932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb08b05",
   "metadata": {},
   "source": [
    "## Carregando a LLM da Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ddbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name='llama-3.3-70b-versatile',temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423f0b5",
   "metadata": {},
   "source": [
    "## Carregamento e Processamento dos Documentos PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cad1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'guia'\n",
    "filename = 'guia_viagens.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a4bbb",
   "metadata": {},
   "source": [
    "### Leitura do Documento PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b71cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "file_path = os.path.join(folder_path, filename)\n",
    "loader = PyPDFLoader(file_path)\n",
    "document.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77274bda",
   "metadata": {},
   "source": [
    "## Geração de Embeddings e Armazenamento no Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f250bf1",
   "metadata": {},
   "source": [
    "### Divisão dos Documentos em Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce2b9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2895aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks: 28\n"
     ]
    }
   ],
   "source": [
    "print(f'Total de chunks: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66eb99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'LibreOffice 25.2.3.2 (X86_64) / LibreOffice Community', 'creator': 'PyPDF', 'creationdate': '2025-09-09T18:32:15-07:00', 'title': 'Guia de Viagens', 'source': 'guia\\\\guia_viagens.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='linhas (Azul, Vermelha, Verde e Amarela) e conecta bairros importantes. Elétricos \\n(bondes) como a Linha 28 oferecem trajetos panorâmicos. Ônibus e trens regionais \\ncomplementam a rede. Táxis e aplicativos de transporte são amplamente disponíveis.\\nAcomodação\\nPrincipais áreas para hospedagem incluem Baixa (central e histórica), Chiado (bairro \\nelegante com lojas e cafés), Alfama (mais antigo, com ruas estreitas e ambiente \\ntradicional) e Bairro Alto (conhecido pela vida noturna). Cascais e Estoril são opções \\nlitorâneas próximas.\\nPrincipais Atrações\\n• Torre de Belém: Fortificação do século XVI em estilo manuelino às margens do \\nTejo; oferece vista panorâmica e detalhes ornamentais como esculturas de \\nrinocerontes e cruzes dos Cavaleiros de Cristo. (Horário: 09:30–18:00 (fechado às')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdee996",
   "metadata": {},
   "source": [
    "### Inicializar modelo de embeddings (HuggingFace para uso local/gratuito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de175d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrador\\AppData\\Local\\Temp\\ipykernel_6444\\3701169180.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fde09",
   "metadata": {},
   "source": [
    "### Inicializar Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1b9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4f4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = None\n",
    "retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1200ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag-viagem\" \n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()] # change if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904730a",
   "metadata": {},
   "source": [
    "### Deletar e recriar o índice para garantir que a dimensão esteja correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b394fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deletando o índice existente 'rag-viagem'...\n"
     ]
    }
   ],
   "source": [
    "if index_name in existing_indexes:\n",
    "    print(f\"Deletando o índice existente '{index_name}'...\")\n",
    "    pc.delete_index(index_name)\n",
    "    time.sleep(1) # Aguardar a exclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b861bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    metric=\"cosine\",\n",
    "    dimension=384, # Isso irá retornar 384\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    ")\n",
    "while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "    print(\"Aguardando o índice ficar pronto...\"+pc.describe_index(index_name).status)\n",
    "    time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86814cca",
   "metadata": {},
   "source": [
    "### Criar ou conectar ao VectorStore do Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a126ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e109f1",
   "metadata": {},
   "source": [
    "## RouterChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7bb9f4",
   "metadata": {},
   "source": [
    "\n",
    "### Prompt templates para as cadeias\n",
    "Prompts específicos por intenção, com **RAG** quando aplicável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a3d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_system = (\n",
    "    \"Você é um assistente de viagens que responde em português do Brasil, com tom claro e útil. \"\n",
    "    \"Seja conciso, mas completo. Cite fatos apenas quando aparecerem no contexto.\"\n",
    ")\n",
    "\n",
    "prompt_intinerary = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", base_system + \" Gere um roteiro detalhado, organizado por dias e períodos. \"\n",
    "     \"Considere perfil do viajante (cultural, gastronômico, aventura), duração, cidade e preferências.\"),\n",
    "    (\"system\", \"Contexto recuperado (RAG):\\n{context}\"),\n",
    "    (\"human\", \"Gere um roteiro para: {query}. Inclua dicas práticas e alternativas em caso de chuva.\")\n",
    "])\n",
    "\n",
    "prompt_logistics = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", base_system + \" Responda sobre transporte, acomodação, deslocamentos e custos aproximados quando possível.\"),\n",
    "    (\"system\", \"Contexto recuperado (RAG):\\n{context}\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "prompt_localinfo = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", base_system + \" Forneça informações específicas sobre atrações, horários, restaurantes e eventos.\"),\n",
    "    (\"system\", \"Contexto recuperado (RAG):\\n{context}\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "propmpt_translation = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", base_system + \" Você é um guia de tradução para viagens. \"\n",
    "     \"Forneça frases úteis traduzidas e transliterações quando cabível.\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d8028",
   "metadata": {},
   "source": [
    "### Cadeias especializadas\n",
    "Cada cadeia recebe `{query}` e (quando aplicável) `context` vindo do **retriever**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f15447fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mensagem padrão caso não haja dados suficientes no RAG\n",
    "mensagem_falta_dado = (\n",
    "    \"Desculpe, não encontrei essa informação na minha base de dados vetorizada (RAG). \"\n",
    "    \"Sugiro realizar uma **nova atualização do RAG** com documentos que contenham esse conteúdo.\"\n",
    ")\n",
    "\n",
    "# Instruções do sistema para limitar respostas ao contexto\n",
    "system_instrucao_pt = (\n",
    "    \"Você é um assistente que **somente** pode responder com base no CONTEXTO fornecido abaixo.\\n\"\n",
    "    \"Se a resposta não estiver no contexto, diga explicitamente que **não consta na base de dados** \"\n",
    "    \"e **sugira uma atualização do RAG**. Não invente detalhes fora do contexto.\\n\\n\"\n",
    "    \"Regras:\\n\"\n",
    "    \"1) Use apenas fatos presentes no CONTEXTO.\\n\"\n",
    "    \"2) Se faltar evidência suficiente, responda com a mensagem padrão de ausência.\\n\"\n",
    "    \"3) Seja conciso e cite trechos do contexto quando útil.\\n\"\n",
    ")\n",
    "\n",
    "# Template da mensagem do usuário com contexto\n",
    "user_template_pt = (\n",
    "    \"PERGUNTA:\\n{question}\\n\\n\"\n",
    "    \"CONTEXTO (trechos do RAG):\\n{context}\\n\\n\"\n",
    "    \"Responda **apenas** com base no CONTEXTO. Se não estiver no CONTEXTO, \"\n",
    "    \"diga que não consta na base e sugira atualização do RAG.\"\n",
    ")\n",
    "\n",
    "def _format_context(docs: List[Document], max_chars: int = 4000) -> str:\n",
    "    partes = []\n",
    "    total = 0\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        trecho = d.page_content.strip()\n",
    "        fonte = d.metadata.get('source') if isinstance(d.metadata, dict) else None\n",
    "        header = f'[{i}] Fonte: {fonte}\\n' if fonte else f'[{i}]\\n'\n",
    "        bloco = header + trecho + '\\n'\n",
    "        if total + len(bloco) > max_chars:\n",
    "            break\n",
    "        partes.append(bloco)\n",
    "        total += len(bloco)\n",
    "    return '\\n'.join(partes).strip()\n",
    "\n",
    "def _tem_evidencia_suficiente(\n",
    "    docs: List[Document],\n",
    "    min_hits: int = 1,\n",
    "    min_chars: int = 200,\n",
    "    score_key: Optional[str] = 'score',\n",
    "    min_score: float = 0.3,\n",
    ") -> bool:\n",
    "    if not docs or len(docs) < min_hits:\n",
    "        return False\n",
    "    texto_total = sum(len(getattr(d, 'page_content', '') or '') for d in docs)\n",
    "    if score_key:\n",
    "        try:\n",
    "            scores = []\n",
    "            for d in docs:\n",
    "                meta = getattr(d, 'metadata', {}) or {}\n",
    "                val = meta.get(score_key)\n",
    "                if isinstance(val, (int, float)):\n",
    "                    scores.append(float(val))\n",
    "                elif isinstance(val, list) and val and isinstance(val[0], (int, float)):\n",
    "                    scores.append(float(val[0]))\n",
    "            if scores and max(scores) < min_score:\n",
    "                return False\n",
    "        except Exception:\n",
    "            pass\n",
    "    return texto_total >= min_chars or len(docs) >= max(min_hits, 2)\n",
    "\n",
    "def responder_rag(\n",
    "    pergunta: str,\n",
    "    retriever: Any,\n",
    "    llm: Any,\n",
    "    router_chain: Optional[Any] = None,\n",
    "    k: int = 4,\n",
    "    min_hits: int = 1,\n",
    "    min_chars: int = 200,\n",
    "    score_key: Optional[str] = 'score',\n",
    "    min_score: float = 0.3,\n",
    "    system_prefix: str = system_instrucao_pt,\n",
    "    user_template: str = user_template_pt,\n",
    ") -> str:\n",
    "    # Responde somente com base no RAG. Se não houver evidência suficiente,\n",
    "    # retorna a mensagem padrão de ausência. Router_chain é ignorado aqui.\n",
    "    try:\n",
    "        if hasattr(retriever, 'get_relevant_documents'):\n",
    "            docs = retriever.get_relevant_documents(pergunta)[:k]\n",
    "        else:\n",
    "            docs = retriever.invoke(pergunta)\n",
    "            if isinstance(docs, list):\n",
    "                docs = docs[:k]\n",
    "            else:\n",
    "                docs = [docs]\n",
    "    except Exception as e:\n",
    "        return f\"{mensagem_falta_dado} (Falha no retriever: {e})\"\n",
    "\n",
    "    if not isinstance(docs, list):\n",
    "        docs = list(docs) if docs else []\n",
    "\n",
    "    if not _tem_evidencia_suficiente(docs, min_hits, min_chars, score_key, min_score):\n",
    "        return mensagem_falta_dado\n",
    "\n",
    "    contexto = _format_context(docs)\n",
    "    user_msg = user_template.format(question=pergunta, context=contexto)\n",
    "\n",
    "    try:\n",
    "        final_prompt = f\"{system_prefix}\\n\\n{user_msg}\"\n",
    "        resposta = llm.invoke(final_prompt)\n",
    "        content = getattr(resposta, 'content', None) or getattr(getattr(resposta, 'message', None) or object(), 'content', None)\n",
    "        if content:\n",
    "            return content\n",
    "        return str(resposta)\n",
    "    except Exception as e:\n",
    "        return f\"{mensagem_falta_dado} (Falha no LLM: {e})\"\n",
    "\n",
    "# Funções de cadeia específicas, todas baseadas em responder_rag\n",
    "\n",
    "def itinerary_chain(query: str) -> str:\n",
    "    return responder_rag(pergunta=query, retriever=retriever, llm=llm)\n",
    "\n",
    "def logistics_chain(query: str) -> str:\n",
    "    return responder_rag(pergunta=query, retriever=retriever, llm=llm)\n",
    "\n",
    "def localinfo_chain(query: str) -> str:\n",
    "    return responder_rag(pergunta=query, retriever=retriever, llm=llm)\n",
    "\n",
    "def translation_chain(query: str) -> str:\n",
    "    return responder_rag(pergunta=query, retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a539447",
   "metadata": {},
   "source": [
    "### Router (classificador de intenção)\n",
    "O roteador decide entre as cadeias: `roteiro-viagem`, `logistica-transporte`, `info-local`, `traducao-idiomas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a7a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Você é um roteador de intenções. Dado o texto do usuário, responda com um JSON válido com as chaves:\n",
    "    - route: uma das opções [\"roteiro-viagem\", \"logistica-transporte\", \"info-local\", \"traducao-idiomas\"]\n",
    "    - reasoning: breve justificativa (1–2 frases)\n",
    "    - normalized_query: reescreva a consulta de forma clara e completa\n",
    "\n",
    "    Exemplos:\n",
    "    - \"roteiro cultural em Paris por 3 dias\" -> route=\"roteiro-viagem\"\n",
    "    - \"como chegar ao Coliseu?\" -> route=\"logistica-transporte\"\n",
    "    - \"horário do Louvre e preço\" -> route=\"info-local\"\n",
    "    - \"frases básicas em japonês\" -> route=\"traducao-idiomas\"\n",
    "\n",
    "    Usuário: {query}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def route_query(query: str) -> Dict[str, Any]:\n",
    "    prompt = router_prompt.format(query=query)\n",
    "    raw = (llm | StrOutputParser()).invoke(prompt)\n",
    "    \n",
    "    # tentativa robusta de parse\n",
    "    try:\n",
    "        # Captura primeiro bloco JSON\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", raw)\n",
    "        data = json.loads(m.group(0) if m else raw)\n",
    "    except Exception:\n",
    "        # fallback mínimo\n",
    "        data = {\"route\": \"info-local\", \"reasoning\": \"fallback\", \"normalized_query\": query}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dd416",
   "metadata": {},
   "source": [
    "### Orquestração\n",
    "Mapeia a rota => executa a cadeia correspondente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd6eab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_to_chain = {\n",
    "    \"roteiro-viagem\": itinerary_chain,\n",
    "    \"logistica-transporte\": logistics_chain,\n",
    "    \"info-local\": localinfo_chain,\n",
    "    \"traducao-idiomas\": translation_chain,\n",
    "}\n",
    "\n",
    "def routerchain_invoke(query: str) -> Dict[str, Any]:\n",
    "    decision = route_query(query)\n",
    "    route = decision.get(\"route\", \"info-local\")\n",
    "    norm_q = decision.get(\"normalized_query\", query)\n",
    "    chain = route_to_chain.get(route, localinfo_chain)\n",
    "    answer = chain(norm_q)\n",
    "    return {\n",
    "        \"route\": route,\n",
    "        \"normalized_query\": norm_q,\n",
    "        \"answer\": answer,\n",
    "        \"router_reasoning\": decision.get(\"reasoning\", \"\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fa6ea",
   "metadata": {},
   "source": [
    "### Testes rápidos\n",
    "Executa alguns exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92ce8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Pergunta: roteiro cultural na cidade de Paris por 3 dias\n",
      "Rota: roteiro-viagem\n",
      "— Router: A consulta menciona um 'roteiro cultural' e um período específico de tempo ('3 dias') em uma cidade específica ('Paris'), o que sugere que o usuário está procurando por um plano de viagem.\n",
      "\n",
      "Resposta:\n",
      " Para um roteiro cultural na cidade de Paris para um período de 3 dias, podemos seguir o perfil cultural descrito no contexto:\n",
      "\n",
      "Dia 1: Visita à Catedral de Notre-Dame e Sainte-Chapelle.\n",
      "Dia 2: Torre Eiffel, Museu Orsay, Arco do Triunfo e caminhada pela Champs-Élysées.\n",
      "Dia 3: Palácio de Versalhes ou viagem de um dia a Giverny (casa de Monet).\n",
      "\n",
      "Essas informações estão presentes no contexto fornecido, especificamente na seção relacionada a Paris. Não há mais detalhes sobre outros roteiros culturais específicos para Paris além disso. Se mais informações forem necessárias, sugiro uma atualização do RAG para incluir mais detalhes sobre roteiros culturais em Paris. \n",
      "\n",
      "================================================================================\n",
      "Pergunta: qual o horário de funcionamento da Catedral de Notre-Dame?\n",
      "Rota: info-local\n",
      "— Router: A pergunta busca informações específicas sobre o horário de funcionamento de um local turístico, a Catedral de Notre-Dame.\n",
      "\n",
      "Resposta:\n",
      " O horário de funcionamento da Catedral de Notre-Dame em Paris é de 07:45–18:45, com entrada gratuita. \n",
      "\n",
      "================================================================================\n",
      "Pergunta: quais são os restaurantes indicado de Paris?\n",
      "Rota: info-local\n",
      "— Router: A pergunta busca informações sobre restaurantes em Paris, o que se encaixa na categoria de informações locais.\n",
      "\n",
      "Resposta:\n",
      " Os restaurantes recomendados em Paris são:\n",
      "- Le Clarence: Restaurante gastronômico premiado com duas estrelas Michelin;\n",
      "- Boulangerie Du Pain et des Idées: Padaria famosa por viennoiseries;\n",
      "- Le Comptoir du Relais: Bistrô em Saint-Germain com pratos tradicionais de bistrô. \n",
      "\n",
      "================================================================================\n",
      "Pergunta: frases úteis para utilizar em Paris\n",
      "Rota: traducao-idiomas\n",
      "— Router: A consulta solicita frases úteis para utilizar em Paris, o que implica na necessidade de traduções ou expressões em francês, justificando a rota de tradução de idiomas.\n",
      "\n",
      "Resposta:\n",
      " Existem algumas frases básicas em francês listadas no contexto, que podem ser úteis durante uma viagem a Paris:\n",
      "\n",
      "- Olá – Hello / Bonjour \n",
      "- Por favor – Please / S’il vous plaît \n",
      "- Obrigado(a) – Thank you / Merci \n",
      "- Quanto custa? – How much does it cost? / Combien ça coûte? \n",
      "- Onde fica o metrô? – Where is the metro? / Où est le métro? \n",
      "- Eu gostaria de... – I would like... / Je voudrais... \n",
      "- Você fala inglês? – Do you speak English? / Parlez-vous anglais?\n",
      "\n",
      "Essas frases podem ser encontradas nas seções de \"Guia de Tradução (Frases Úteis)\" nos trechos [1] e [2] do contexto. \n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"roteiro cultural na cidade de Paris por 3 dias\",\n",
    "    \"qual o horário de funcionamento da Catedral de Notre-Dame?\",\n",
    "    \"quais são os restaurantes indicado de Paris?\",\n",
    "    \"frases úteis para utilizar em Paris\",\n",
    "]\n",
    "\n",
    "for q in tests:\n",
    "    print(\"\\n\"+\"=\"*80)\n",
    "    print(\"Pergunta:\", q)\n",
    "    result = routerchain_invoke(q)\n",
    "    print(\"Rota:\", result[\"route\"])\n",
    "    print(\"— Router:\", result[\"router_reasoning\"])\n",
    "    print(\"\\nResposta:\\n\", result[\"answer\"][:1500], \"...\" if len(result[\"answer\"])>1500 else \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
